# -*- coding: utf-8 -*-
"""제6회 2024 연구개발특구 AI SPARK 챌린지 baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/choijhyeok/Collection-of-presentation-materials/blob/main/%EC%A0%9C6%ED%9A%8C%202024%20%EC%97%B0%EA%B5%AC%EA%B0%9C%EB%B0%9C%ED%8A%B9%EA%B5%AC%20AI%20SPARK%20%EC%B1%8C%EB%A6%B0%EC%A7%80%20baseline.ipynb

# BaseLine

- 참가자 분들의 이해를 돕기위한 baseline 입니다.
- 해당 코드는 이해를 위한 참고용입니다.
- 반드시 결과는 해당코드 아래의 **제출 Predict** 부분을 확인하시고 해당 방법처럼 제출해 주시면 감사하겠습니다.

&nbsp;
"""

import os
import warnings
warnings.filterwarnings("ignore")
import glob
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import tensorflow as tf
import keras
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.python.keras import backend as K
import sys
import pandas as pd
from tqdm import tqdm
from keras.preprocessing.image import ImageDataGenerator
import threading
import random
import rasterio
import os
import numpy as np
import sys
from sklearn.utils import shuffle as shuffle_lists
from keras.models import *
from keras.layers import *
import numpy as np
from keras import backend as K
from sklearn.model_selection import train_test_split
import joblib
import time
import datetime
from keras.layers import Attention
from keras.applications import EfficientNetV2B0
dt = datetime.datetime.now()

"""&nbsp;

## 사용할 함수 정의
"""

MAX_PIXEL_VALUE = 65535 # 이미지 정규화를 위한 픽셀 최대값

class threadsafe_iter:
    """
    데이터 불러올떼, 호출 직렬화
    """
    def __init__(self, it):
        self.it = it
        self.lock = threading.Lock()

    def __iter__(self):
        return self

    def __next__(self):
        with self.lock:
            return self.it.__next__()


def threadsafe_generator(f):
    def g(*a, **kw):
        return threadsafe_iter(f(*a, **kw))

    return g

def get_img_arr(path):
    img = rasterio.open(path).read().transpose((1, 2, 0))
    img = np.float32(img)/MAX_PIXEL_VALUE

    return img

def get_img_762bands(path):
    img = rasterio.open(path).read((7,6,2)).transpose((1, 2, 0))
    img = np.float32(img)/MAX_PIXEL_VALUE

    return img

def get_mask_arr(path):
    img = rasterio.open(path).read().transpose((1, 2, 0))
    seg = np.float32(img)
    return seg



@threadsafe_generator
def generator_from_lists(images_path, masks_path, batch_size=32, shuffle = True, random_state=None, image_mode='10bands'):

    images = []
    masks = []

    fopen_image = get_img_arr
    fopen_mask = get_mask_arr

    if image_mode == '762':
        fopen_image = get_img_762bands

    i = 0
    # 데이터 shuffle
    while True:

        if shuffle:
            if random_state is None:
                images_path, masks_path = shuffle_lists(images_path, masks_path)
            else:
                images_path, masks_path = shuffle_lists(images_path, masks_path, random_state= random_state + i)
                i += 1


        for img_path, mask_path in zip(images_path, masks_path):

            img = fopen_image(img_path)
            mask = fopen_mask(mask_path)
            images.append(img)
            masks.append(mask)

            if len(images) >= batch_size:
                yield (np.array(images), np.array(masks))
                images = []
                masks = []

# Unet 모델 정의
def FCN(nClasses, input_height=128, input_width=128, n_filters = 16, dropout = 0.1, batchnorm = True):


    img_input = Input(shape=(input_height,input_width, 3))

    ## Block 1
    x = Conv2D(n_filters, (3, 3), activation='swish', padding='same', name='block1_conv1')(img_input)
    x = Attention()(x)
    x = Conv2D(n_filters, (3, 3), activation='swish', padding='same', name='block1_conv2')(x)
    x = Attention()(x)
    f1 = x

    # Block 2
    x = Conv2D(n_filters, (3, 3), activation='swish', padding='same', name='block2_conv1')(x)
    x = Attention()(x)
    x = Conv2D(n_filters, (3, 3), activation='swish', padding='same', name='block2_conv2')(x)
    x = Attention()(x)
    f2 = x

    # Out
    o = (Conv2D(nClasses, (3,3), activation='swish' , padding='same', name="Out"))(x)
    o = Attention()(o)

    
    model = Model(img_input, o)

    return model


def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):
    # first layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer="he_normal",
               padding="same")(input_tensor)
    a1 = Attention()(x)  # Attention 계산
    x = Multiply()([x, a1])  # Attention을 출력에 곱함
    
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation("swish")(x)

    # second layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer="he_normal",
               padding="same")(x)
    a1 = Attention()(x)  # Attention 계산
    x = Multiply()([x, a1])  # Attention을 출력에 곱함
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation("swish")(x)
    return x

#############################################모델################################################
def conv_block(inputs, num_filters):
    x = Conv2D(num_filters, 3, padding="same")(inputs)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    x = Conv2D(num_filters, 3, padding="same")(x)
    x = BatchNormalization()(x)
    x = Activation("relu")(x)

    return x

# import tensorflow_hub as hub

# # EfficientNetV2B0 모델의 URL
# model_url = "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2"

# # TensorFlow Hub에서 모델 로드
# efficientnetv2b0_model = hub.load(model_url)

# # 모델의 summary 출력
# efficientnetv2b0_model.summary()

    

def decoder_block(inputs, skip, num_filters):
    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding="same")(inputs)
    
    # skip 특징 맵과 업샘플링된 특징 맵의 크기를 조정
    x = tf.image.resize(x, tf.shape(skip)[1:3], method='nearest')
    
    x = Concatenate()([x, skip])
    x = conv_block(x, num_filters)
    return x

def build_effienet_unet(input_shape):
    """ Input """
    inputs = Input(input_shape)

    """ Pre-trained Encoder """
    encoder = EfficientNetV2B0(include_top=False, weights="imagenet", input_tensor=inputs)

    s1 = encoder.get_layer("input_1").output                      ## 256
    s2 = encoder.get_layer("block2a_expand_activation").output    ## 128
    s3 = encoder.get_layer("block3a_expand_activation").output    ## 64
    s4 = encoder.get_layer("block4a_expand_activation").output    ## 32

    """ Bottleneck """
    b1 = encoder.get_layer("block6a_expand_activation").output    ## 16

    """ Decoder """
    d1 = decoder_block(b1, s4, 256)                               ## 32
    d2 = decoder_block(d1, s3, 128)                               ## 64
    d3 = decoder_block(d2, s2, 64)                               ## 128
    d4 = decoder_block(d3, s1, 32)                                ## 256

    """ Output """
    outputs = Conv2D(1, 1, padding="same", activation="sigmoid")(d4)

    model = Model(inputs, outputs, name="EfficientNetV2B0_UNET")
    return model

if __name__ == "__main__":
    input_shape = (256, 256, 3)
    model = build_effienet_unet(input_shape)
    model.summary()
###########################################################################

# 두 샘플 간의 유사성 metric
def dice_coef(y_true, y_pred, smooth=1):
    intersection = K.sum(y_true * y_pred, axis=[1,2,3])
    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])
    dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)
    return dice

# 픽셀 정확도를 계산 metric
def pixel_accuracy (y_true, y_pred):
    sum_n = np.sum(np.logical_and(y_pred, y_true))
    sum_t = np.sum(y_true)

    if (sum_t == 0):
        pixel_accuracy = 0
    else:
        pixel_accuracy = sum_n / sum_t
    return pixel_accuracy

"""&nbsp;

## parameter 설정
"""

# 사용할 데이터의 meta정보 가져오기

train_meta = pd.read_csv('C:\\dataset\\train_meta.csv')
test_meta = pd.read_csv('C:\\dataset\\test_meta.csv')


# 저장 이름
save_name = f'_{dt.day}day{dt.hour:2}{dt.minute:2}'

N_FILTERS = 16 # 필터수 지정
N_CHANNELS = 3 # channel 지정
EPOCHS = 1 # 훈련 epoch 지정
BATCH_SIZE = 8 # batch size 지정
IMAGE_SIZE = (256, 256) # 이미지 크기 지정
MODEL_NAME = 'unet' # 모델 이름
RANDOM_STATE = 730501 # seed 고정
INITIAL_EPOCH = 0 # 초기 epoch

# 데이터 위치
IMAGES_PATH = 'C:\\dataset\\train_img\\'
MASKS_PATH = 'C:\\dataset\\train_mask\\'

# 가중치 저장 위치
OUTPUT_DIR = 'C:\\dataset\\output\\'
WORKERS = 4

# 조기종료
EARLY_STOP_PATIENCE = 100

# 중간 가중치 저장 이름
CHECKPOINT_PERIOD = 10
CHECKPOINT_MODEL_NAME = 'checkpoint-{}-{}-epoch_{{epoch:02d}}.hdf5'.format(MODEL_NAME, save_name)

# 최종 가중치 저장 이름
FINAL_WEIGHTS_OUTPUT = 'model_{}_{}_final_weights.h5'.format(MODEL_NAME, save_name)

# 사용할 GPU 이름
CUDA_DEVICE = 0


# 저장 폴더 없으면 생성
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)


# GPU 설정
os.environ["CUDA_VISIBLE_DEVICES"] = str(CUDA_DEVICE)
try:
    config = tf.compat.v1.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.compat.v1.Session(config=config)
    K.set_session(sess)
except:
    pass

try:
    np.random.bit_generator = np.random._bit_generator
except:
    pass


# train : val = 8 : 2 나누기
x_tr, x_val = train_test_split(train_meta, test_size=0.15, random_state=RANDOM_STATE)
print(len(x_tr), len(x_val))

# train : val 지정 및 generator
images_train = [os.path.join(IMAGES_PATH, image) for image in x_tr['train_img'] ]
masks_train = [os.path.join(MASKS_PATH, mask) for mask in x_tr['train_mask'] ]

images_validation = [os.path.join(IMAGES_PATH, image) for image in x_val['train_img'] ]
masks_validation = [os.path.join(MASKS_PATH, mask) for mask in x_val['train_mask'] ]

train_generator = generator_from_lists(images_train, masks_train, batch_size=BATCH_SIZE, random_state=RANDOM_STATE, image_mode="762")
validation_generator = generator_from_lists(images_validation, masks_validation, batch_size=BATCH_SIZE, random_state=RANDOM_STATE, image_mode="762")

#miou metric
def miou(y_true, y_pred, smooth=1e-6):
    # 임계치 기준으로 이진화
    THESHOLDS = 0.25
    y_pred = tf.cast(y_pred > THESHOLDS, tf.float32)
    
    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])
    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3]) - intersection
    
    # mIoU 계산
    iou = (intersection + smooth) / (union + smooth)
    miou = tf.reduce_mean(iou)
    return miou


model.compile(optimizer = Adam(learning_rate= 0.01), loss = 'binary_crossentropy', metrics = ['accuracy', miou])
model.summary()


# checkpoint 및 조기종료 설정 val_miou 기준
es = EarlyStopping(monitor='val_miou', mode='max', verbose=1, patience = 20 , restore_best_weights=True)
checkpoint = ModelCheckpoint(os.path.join(OUTPUT_DIR, CHECKPOINT_MODEL_NAME), monitor='val_miou', verbose=1,
save_best_only=True, mode='max', period=CHECKPOINT_PERIOD)
rlr = ReduceLROnPlateau(monitor='val_loss',
                        patience= 10,
                        mode= 'auto',
                        factor= 0.1,
                        verbose=1)

""" # checkpoint 및 조기종료 설정  val_loss 기준
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 20 , restore_best_weights=True)
checkpoint = ModelCheckpoint(os.path.join(OUTPUT_DIR, CHECKPOINT_MODEL_NAME), monitor='val_loss', verbose=1,
save_best_only=True, mode='min', period=CHECKPOINT_PERIOD)
rlr = ReduceLROnPlateau(monitor='val_loss',
                        patience= 10,
                        mode= 'auto',
                        factor= 0.1,
                        verbose=1) """


"""&nbsp;

## model 훈련
"""
start = time.time()
print('---model 훈련 시작---')
history = model.fit_generator(
    train_generator,
    steps_per_epoch=len(images_train) // BATCH_SIZE,
    validation_data=validation_generator,
    validation_steps=len(images_validation) // BATCH_SIZE,
    callbacks=[checkpoint, es],
    epochs=EPOCHS,
    workers=WORKERS,
    initial_epoch=INITIAL_EPOCH
)
print('---model 훈련 종료---')
end = time.time()

"""&nbsp;

## model save
"""

print('가중치 저장')
model_weights_output = os.path.join(OUTPUT_DIR, FINAL_WEIGHTS_OUTPUT)
model.save_weights(model_weights_output)
print("저장된 가중치 명: {}".format(model_weights_output))

"""## inference

- 학습한 모델 불러오기
"""

# model = get_model(MODEL_NAME, input_height=IMAGE_SIZE[0], input_width=IMAGE_SIZE[1], n_filters=N_FILTERS, n_channels=N_CHANNELS)
model.compile(optimizer = Adam(), loss = 'binary_crossentropy', metrics = ['accuracy'])
model.summary()

df = pd.DataFrame(train_generator , columns = train_generator.feature_names)

print('=================== 상관계수 히트맵 =====================')
print(df.corr())                                 # correlation
#                    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target(Y)
# sepal length (cm)           1.000000         -0.117570           0.871754          0.817941   0.782561
# sepal width (cm)           -0.117570          1.000000          -0.428440         -0.366126  -0.426658
# petal length (cm)           0.871754         -0.428440           1.000000          0.962865   0.949035
# petal width (cm)            0.817941         -0.366126           0.962865          1.000000   0.956547
# target(Y)                   0.782561         -0.426658           0.949035          0.956547   1.000000

# -0.1 이 0보다 좋다
# 다중공섬선?
# 상관관계가 너무 높은 애들을 가지쳐주는게 성능이 좋을수도 있다. 높은애들만 있으면 걔들한테만 과적합되어서 상관관계가 없는 애들이 피해를 봄
# y값과 상관관계가 있는게 좋은 데이터 , x값과 상관관계가 있는게 애매한 데이터


import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
print(sns.__version__)
print(matplotlib.__version__)               # 3.8.0
sns.set(font_scale=1.2)
sns.heatmap(data=df.corr(), 
            square=True,    
            annot=True,                       # 표안에 수치 명시
            cbar=True)                        # 사이드 바
plt.show()


"""## 제출 Predict
- numpy astype uint8로 지정
- 반드시 pkl로 저장

"""
""" 
model.load_weights('C:/dataset/output/model_unet__11day12 8_final_weights.h5')

y_pred_dict = {}

for i in test_meta['test_img']:
    img = get_img_762bands(f'C:/dataset/test_img/{i}')
    y_pred = model.predict(np.array([img]), batch_size=1)

    y_pred = np.where(y_pred[0, :, :, 0] > 0.25, 1, 0) # 임계값 처리
    y_pred = y_pred.astype(np.uint8)
    y_pred_dict[i] = y_pred


joblib.dump(y_pred_dict, f'C:/dataset/output/submit_{dt.day}day{dt.hour:2}{dt.minute:2}.pkl')

print('피클 저장')
# print(end - start) """